% ==========================================
% 2. BAKGRUND [cite: 56, 185]
% ==========================================
\section*{Bakgrund}
\addcontentsline{toc}{section}{Bakgrund}
\vspace{0.5cm}



\noindent
Med detta avsnitt introducerar vi de begrepp som kommer användas framöver i vår studie. Först beskriver vi hur bildkomprimering sker traditionellt.
Därefter förklarar vi autoencoders och variational autoencoders, följt av SDXL, den specifika variant som används i studien. Vidare ges en beskrivning av nödvändiga begrepp såsom kvantisering samt kvalitetsmåtten som används för att
utvärdera rekonstruktioner.

\subsection*{Traditionell bildkomprimering}
...på gång...

\noindent

\subsection*{Autoencoders, VAE och latenta representationer}
En \textit{autoencoder} är ett djupt neuralt nätverk vars syfte är att lära sig att representera data i en komprimerad form och därefter rekonstruera den ursprungliga datan med så liten förlust som möjligt.
Nätverket består av två delar, en \textit{encoder} som omvandlar källdatan till en kompakt representation, och en \textit{decoder} som återskapar datan från denna representation.
Genom att ett mellanliggande lager i nätverket har lägre dimension än källan uppstår en så kallad \textit{flaskhals}, vilket tvingar nätverket att identifiera och bevara de mest väsentliga egenskaperna i datan. Den kompakta representationen som uppstår i flaskhalsen kallas för en \textit{latent representation}, eller \textit{latent space}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth, page=1]{images/autoencoder.pdf}
    \caption{Autoencoder med encoder, flaskhals och decoder.}
\end{figure}

En begränsning med vanliga autoencoders är att det latenta rummet saknar en kontinuerligt organiserad struktur. Närliggande punkter i det latenta rummet behöver inte motsvara semantiskt liknande data, vilket gör det svårt att använda representationen för generativa ändamål.
\emph{<GE EXEMPEL>}

En \textit{variational autoencoder} (VAE) löser detta genom att encodern inte unikt omvandlar varje datapunkt till en enskild punkt i det latenta rummet, utan istället till en sannolikhetsfördelning~\cite{kingma2022autoencodingvariationalbayes}. Encodern producerar två värden för varje latent dimension, ett medelvärde som anger fördelningens centrum och en varians som anger hur utspridd fördelningen är kring detta centrum. Från denna fördelning samplas sedan en latent vektor som skickas vidare till decodern \emph{<HÄNVISA TILL FIGUR>}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.75\textwidth, page=1]{images/vae_tikz_param-2.pdf}
    \caption{Variational autoencoder med reparameterisering.}
\end{figure}

%För att modellen ska kunna tränas med gradientbaserade metoder, trots att samplingen i sig inte är deriverbar, används det så kallade \textit{reparameteriseringstricket}. Istället för att sampla den latenta vektorn direkt från fördelningen, beräknas den genom att addera en slumpmässig störning, skalad med variansen, till medelvärdet. På detta sätt kan gradienter fortfarande beräknas genom hela nätverket~\cite{kingma2022autoencodingvariationalbayes}.

Vid träning optimeras VAE:n med två samtidiga mål. Det första målet är att minimera skillnaden mellan källdata och rekonstruerad data, det vill säga att bevara så mycket information som möjligt. Det andra målet är att den inlärda latenta fördelningen ska ligga nära en fördefinierad referensfördelning. Avvikelsen mellan dessa fördelningar mäts och minimeras. Detta gör att det latenta rummet blir mer kontinuerligt och välordnat så att närliggande punkter i högre grad motsvarar semantiskt liknande innehåll~\cite{kingma2022autoencodingvariationalbayes}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{images/oversimplified.jpg}
    \caption{<HÄNVISA TILL KÄLLAN>}
    \label{fig:placeholder}
\end{figure}

%En konsekvens av detta är att rekonstruktionerna ibland blir något suddigare jämfört med en autoencoder, eftersom modellen måste balansera  en korrekt rekonstruktion mot ett strukturerat/kontinuerligt latent rum. Dock är detta  just den egenskap som VAE särskilt lämpar sig för i  generativa modeller där man vill kunna bearbeta data i ett kompakt och välorganiserat utrymme.

\subsection*{SD/SDXL och dess VAE}
SD/SDXL (\textit{Stable Diffusion/XL}) är en latent diffusionsmodell utvecklad för att generera högupplösta bilder~\cite{podell2024sdxl}. Till skillnad från tidigare generationers diffusionsmodeller som arbetade direkt med pixelvärden utför SD/SDXL hela diffusionsprocessen i ett latent rum, vilket avsevärt minskar antalet beräkningar som behövs. 

Omvandlingen mellan pixelvärden och det latenta rummet sker med hjälp av en VAE. Encodern i denna VAE tar en bild med upplösningen 1024\texttimes1024\texttimes3 och komprimerar den till en representation med dimensionen 128\texttimes128\texttimes4, vilket innebär att bildens höjd och bredd komprimeras med en faktor av 8 i vardera riktning~\cite{podell2024sdxl}. Samtidigt ökas antalet kanaler från 3 (RGB) till 4 dimensioner. Dessa fyra kanaler representerar inte längre färger, utan kallas 'features'. 

Den latenta representation kan sedan användas i efterföljande diffusionsprocess som stegivs lägger till eller tar bort brus för att generera nya bilder. Denna process sker helt i detta komprimerade latenta rum. När processen är klar avkodar VAE:ns decoder den latenta representationen tillbaka till en fullupplöst RGB-bild igen.

Arkitekturen gör det möjligt att generera bilder med hög visuell kvalitet men med en bråkdel av den beräkningskostnad som skulle krävas om diffusionsprocessen utfördes direkt på pixelvärden.

I vår studie är använder vi samma VAE:n fristående för att komprimera befintliga bilder till latenta representationer och sedan rekonstruera dem, utan diffusionsprocessen.

\subsection*{Kvantisering och lagring av latenta representationer}
De latenta representationer som SDXL:s VAE producerar lagras i 16/32-bitars flyttalsformat vilket ger en hög nogrannhet. Dock är det bättre från kompremeringssynpunkt att omvandla dessa till heltal med lägre bitdjup genom en process som kallas \textit{kvantisering}.

Vid kvantisering omvandlas det kontinuerliga intervallet av flyttalsvärden till ett diskret antal heltalsnivåer. Antalet tillgängliga nivåer bestäms av det valda bitdjupet: 8-bitars kvantisering ger 256 nivåer, 10-bitars ger 1\,024 nivåer, 12-bitars ger 4\,096 nivåer och 16-bitars ger 65\,536 nivåer. Genom att representera de kvantiserade värdena som kanaler i en RGBA-bild kan resultatet sedan lagras med konventionella bildformat som AVIF eller PNG.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{images/kvantisering.pdf}
    \caption{Kvantiseringsprocessen: latenta flyttalsvärden omvandlas till heltal och lagras som komprimerad bildfil.}
    \label{fig:kvantisering}
\end{figure}

 Vid kvantisering behöver en avvägning göras mellan att antingen välja färre bitar för mindre filstorlek, eller högre bitdjup med mindre kvalitetsförlust. Hur mycket detta i slutändan påverkar den slutgiltiga bildkvaliteten efter dekodning/rekonstruktion är en av de grundläggande frågorna vi vill undersöka med denna studie eftersom detta är ytterliggare en aspekt vid sidan av VAE:ns egna inverkan på bildkvaliteten.

\subsection*{Kvalitetsmått för rekonstruktion}
För att kvantitativt kunna mäta och utvärdera hur väl en rekonstruerad bild överensstämmer med originalet används i denna studie två etablerade kvalitetsmått, nämligen PSNR och SSIM.

\textit{PSNR} (\textit{Peak Signal-to-Noise Ratio}) mäter den genomsnittliga avvikelsen mellan pixlar i originalbilden och den rekonstruerade bilden och uttrycks i decibel (dB). Ett högre PSNR-värde indikerar en mindre avvikelse och därmed en bättre rekonstruktion~\cite{hore2010psnrssim}. Måttet är enkelt att beräkna och vida använt, men det fångar enbart skillnader på pixelnivå och tar inte hänsyn till hur vi människor  uppfattar bildkvalitet. Även i princip osynliga skillnader kan påverka värdet kraftigt.  \emph{<KÄLLA BEHÖVS>} 

\textit{SSIM} (\textit{Structural Similarity Index}) kompletterar PSNR genom att istället mäta strukturell likhet mellan bilderna. SSIM beaktar tre komponenter: luminans (ljusstyrka), kontrast och strukturell information, och kombinerar dessa till ett övergripande mått~\cite{hore2010psnrssim}. SSIM-värden ligger i intervallet 0 till 1, där 1 innebär identiska bilder. Genom att fokusera på strukturella egenskaper snarare än enskilda pixelvärden ger SSIM ett bättre mått på övergripande likhet. Att använda båda måtten tillsammans är vanligt i bildbehandlingsstudier eftersom de kompletterar varandra och inte mäter samma sak. \emph{<KÄLLA BEHÖVS>} 

%\subsection*{Övergång till studiens frågeställning}


\noindent
